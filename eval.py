import os
import csv
import json
import random
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from PIL import Image
from torchvision import transforms
import rnn

'''
Evaluate the performance of the trained model under different settings:
+ 'eval': held-out object set 처음의 만들었던 dataset
+ 'YCB': YCB object set robotics에 사용되는 새로운 dataset
+ 'robot': objects in the robot's environment, captured by the robot's camera
'''

parser = argparse.ArgumentParser()
parser.add_argument('--model', type=str, default='models/trained-model.pt',
                    help='saved checkpoint')
parser.add_argument('--input_test', type=str, default='data/corpus-test.csv',
                    help='test data')
#below file is generated by ycb.py
parser.add_argument('--input_embedding', type=str, default='data/ycb-object-embedding.csv',
                    help='ycb image embeddings') #object에 해당하는 embedding vector
parser.add_argument('--ycb_vo', type=str, default='data/ycb-verb-object.csv',
                    help='ycb verb-object pairs for testing') #verb-object pairs
parser.add_argument('--image_dir', type=str, default='robot',
                    help='directory for robot images') #로봇 이미지에 대한 디렉토리?
parser.add_argument('--command', type=str, default='An object to contain',
                    help='natural language command for robot') #command
parser.add_argument('--num_layers', type=int, default=1,
                    help='number of layers of model')
parser.add_argument('--rnn_input', type=int, default=128, help='') #rnn input의 크기 128
parser.add_argument('--hidden_dim', type=int, default=64, help='') # ~ hidden_dim
parser.add_argument('--rnn_output', type=int, default=2048, help='') #rnn의 ouput
parser.add_argument('--dropout', type=float, default=0.0, help='') 
parser.add_argument('--ret_num', type=int, default=5, help='')
parser.add_argument('--mode', type=str, default='robot', help='3 possible evaluation modes: YCB, robot, eval') #위 3개의 mode중 어느것을 test할것인지 결정
parser.add_argument('--DEBUG', type=bool, default=False, help='')
parser.add_argument('--verb_only', type=bool, default=True, help='') #verb만 사용할것인지 결정 default는 true
opt = parser.parse_args() #opt라는 이름으로 parameter들을 사용

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
cuda = True if torch.cuda.is_available() else False
Tensor = torch.cuda.LongTensor if cuda else torch.LongTensor


#load resnet model
resnet = models.resnet101(pretrained=True) #resmet101을 사용
#access average pooling layer in network
model_avgpool = nn.Sequential(*list(resnet.children())[:-1]) #마지막 fc layer를 제외한 model_avgpool을 불러옴
model_avgpool.eval()
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


#Load word vocab (generated in run.py)
# run.py를 돌려서 word -> vector / vector -> word로 구성된 dictionary를 가져옴
word2id = None
with open("dict/word2id.json") as f:
    for line in f:
        word2id = json.loads(line)
id2word = None
with open("dict/id2word.json") as f:
    for line in f:
        id2word = json.loads(line)


#Load trained model
#Embedding을 진행후 RNN네트워크
model = nn.Sequential(
nn.Embedding(len(word2id), opt.rnn_input), #(단어 집합의 크기,임베딩 할 벡터의 차원)(word2id의 크기,128)
rnn.RNNModel(opt.rnn_input, opt.rnn_output, opt.hidden_dim, opt.num_layers,
             opt.dropout, device)).to(device) #(128,2048,64,1)
model.load_state_dict(torch.load(opt.model)) #저장된 모델에서 가중치 정보를 가져옴


# Generate natural language command from templates, given verb-object pair
def gen_from_template(verb, obj):
    pre_obj = ['Give me the ', 'Hand me the ', 'Pass me the ', 'Fetch the ',
           'Get the ', 'Bring the ', 'Bring me the ',
           'I need the ', 'I want the ',
           'I need a ', 'I want a ']
    pre_verb = ['An item that can ', 'An object that can ',
           'Give me something that can ', 'Give me an item that can ',
           'Hand me something with which I can ',
           'Give me something with which I can ',
           'Hand me something to ', 'Give me something to ',
           'I want something to ', 'I need something to ']
    if opt.verb_only: # verb만 사용한다면
        template = random.choice(pre_verb)
        sentence = template + verb
    else: # verb + object
        template = random.choice(pre_obj)
        sentence = template + obj + ' to ' + verb
    return sentence


# Map each word in the natural language command to its ID in the vocab
# command가 input으로 들어오면 vector로 변환
def process_command(command, word2id, id2word):
    sentence = []
    s = command.lower().split()
    for word in s:
        if word in word2id:
            sentence.append(word2id[word])
        else:
            sentence.append(word2id['UNK'])
    return sentence


# Generate retrieval tasks (from verb-object pairs) to test the model
def gen_ret(vo_dict, objects, aff_dict, word2id, id2word, exclude):
    ret_set = []
    for verb in vo_dict: #verb에 대해서
        for obj in vo_dict[verb]: #그 verb에 해당하는 obj에 대해서 
            if obj not in exclude: #그 obj가 exclude에 없다면
                #generate language command from the verb-object pair
                # command 생성
                sentence = gen_from_template(verb, obj)
                # command를 vector화
                sentence = process_command(sentence, word2id, id2word)
                l = [sentence]
                #verb에 해당하는 obj추가
                ret_objs = [[obj]+random.choice(aff_dict[obj])] #[obj + [vector와image중 random으로]] 
                all_o = [obj]
                while len(ret_objs) < opt.ret_num: # 4개를 추가
                    o = random.choice(objects) #obj random하게 선택
                    #only sample objects that cannot be paired with the current verb # 현재 verb와 pair되지않는 obj하나 선택
                    #and ensure that the retrieval set has all unique objects # image하나당 하나의 obj
                    #(objects from different classes)
                    if (o not in vo_dict[verb]) and (o not in all_o) and (o not in exclude): #verb와 맞지 않고 + 추가된거 x + exclude에 x
                        ret_objs.append([o]+random.choice(aff_dict[o]))
                        all_o.append(o)
                l.append(ret_objs) #총 5개
                ret_set.append(l) #ret_set은 sentence, ret_objs 로 구성
    return ret_set


# Generate retrieval tasks (from test examples) to test the model
def genRet(test, vo_dict):
    ret_set = []
    for verb, obj, sentence, affordances, img in test:
        l = [sentence]
        #the object included in the current test example
        #is the first candidate object for this retrieval task
        ret_objs = [[obj, affordances, img]]
        all_o = [obj]
        #each retrieval task includes ret_num (5) candidate objects
        #for the model to select from
        while len(ret_objs) < opt.ret_num:
            sample = random.choice(test)
            #only sample objects that cannot be paired with the current verb
            #and make sure that all objects in the retrieval set are unique
            #(from different object classes)
            if (sample[1] not in vo_dict[verb]) and (sample[1] not in all_o):
                ret_objs.append([sample[1], sample[3], sample[4]])
                all_o.append(sample[1])
        l.append(ret_objs)
        ret_set.append(l)
    return ret_set


# Test the model on retrieval tasks (selecting the correct object from a set of 5)
def ret(model, ret_set, id2word):
    model.eval() #evaluation mode
    correct, correct2 = 0.0, 0.0
    with torch.no_grad(): #test라서 gradient 는 false로 
        for sentence, ret_objs in ret_set:
            s = ''
            #vector를 다시 word로 변경
            for i in sentence:
                s += id2word[str(i)] + ' '
            sentence = Tensor(sentence).unsqueeze(0) #0번째 자리에 1인차원 추가
            sims = []
            output = model(sentence)
            for obj_name, obj, img in ret_objs:
                obj = np.fromstring(obj[1:-1], dtype=float, sep=',') #"" 부분제외 => image embedding vector
                affordances = torch.from_numpy(
                    obj).to(device).float().unsqueeze(0)
                sim = F.cosine_similarity(output, affordances) #두 벡터의 cosine similarity를 구함
                sims.append(sim.item()) #결과들을 list에 저장
            #rank each candidate object based on the similarity value between
            #its embedding and the model's output embedding
            #(we want the model's output to be the most similar to the
            #correct object's embedding, as the model will select the object
            #with the embedding most similar to its output)
            sort = sorted(sims, reverse=True)
            #0번째 원소가 기준 
            if sims[0] == sort[0]:
                correct += 1
                correct2 += 1
                result = 'FIRST'
            elif sims[0] == sort[1]:
                correct2 += 1
                result = 'SECOND'
            else:
                result = 'BOTH WRONG'
            if opt.DEBUG:
                print()
                print(result)
            l = []
            for i, lt in enumerate(ret_objs):
                obj_name, aff, img = lt
                l.append([obj_name, sims[i], img])
            top1, top2 = sims.index(sort[0]), sims.index(sort[1])
            t1, t2 = ret_objs[top1][0], ret_objs[top2][0]
            if opt.DEBUG:
                print(s)
                print(output)
                print(l)
                print(t1,',', t2)
        print('RET_ACC Top1: {} Top2: {}'.format(
            correct/len(ret_set), correct2/len(ret_set)))


if opt.mode == 'YCB': #evaluation on YCB dataset
    aff_dict = {}
    with open(opt.input_embedding, 'r') as f: #ycb object의 imput embedding을 가져옴
        data = list(csv.reader(f))
        for row in data:
            obj = str(row[0]).lower() # object
            aff = str(row[1]) # embeding
            img = str(row[2]) # image경로
            if obj not in aff_dict: #해당 object에 aff,img를 value값으로
                aff_dict[obj] = []
            aff_dict[obj].append([aff, img]) 

    vo_dict = {}
    objects = []
    with open(opt.ycb_vo, 'r') as f: #ycb의 verb-obj pair를 가져옴
        data = list(csv.reader(f)) #list로 변환
        for row in data:
            verb = str(row[0]).lower()
            obj = str(row[1]).lower()
            if verb not in vo_dict:
                vo_dict[verb] = []
            if obj not in vo_dict[verb]: 
                vo_dict[verb].append(obj) #verb-object pair 추가
            if obj not in objects: 
                objects.append(obj) #object 추가

    #exclude objects the model has already seen during training
    exclude = ['banana', 'strawberry', 'orange', 'pitcher base', 'plate', 'phillips screwdriver', 'flat screwdriver', 'hammer', 'baseball', 'toy airplane']
    #generate retrieval tasks (from the annotated verb-object pairs
    #and object embeddings from the YCB object set) to test the model
    ret_set = gen_ret(vo_dict, objects, aff_dict, word2id, id2word, exclude) #(verb-obj,그에 해당하는 obj, 학습할 obj의 embedding, word2id, id2word,exclude할 obj)
    ret(model, ret_set, id2word)


elif opt.mode == 'robot': #robot demo
    model.eval()
    with torch.no_grad():
        print(opt.command) #command 출력
        command = process_command(opt.command, word2id, id2word)
        sentence = Tensor(command).unsqueeze(0) #0번째자리에 1차원
        predicted = model(sentence)

        #use the pretrained resnet model to generate embeddings
        #for the object images captured by the robot
        embeddings = []
        for f in os.listdir(opt.image_dir):
            input_image = Image.open(os.path.join(opt.image_dir, f))
            input_tensor = preprocess(input_image)
            input_batch = input_tensor.unsqueeze(0)

            #move the input and model to GPU for speed if available
            if torch.cuda.is_available():
                input_batch = input_batch.to('cuda')
                model_avgpool.to('cuda')

            try:
                output = model_avgpool(input_batch) #average pooling 사용?
            except:
                print('Cannot encode image', os.path.join(opt.image_dir, f))
            output = torch.flatten(output, 1)
            embeddings.append([f, output])

        sims = []
        for _, em in embeddings:
            sim = F.cosine_similarity(predicted, em)
            sims.append(sim.item())
        #rank each candidate object based on the similarity value between
        #its embedding and the model's output embedding, the model
        #will select the object with the embedding most similar to its output
        sort = sorted(sims, reverse=True)
        t1, t2, t3, t4, t5 = sims.index(sort[0]), sims.index(sort[1]), sims.index(sort[2]), sims.index(sort[3]), sims.index(sort[4])
        top1, top2, top3, top4, top5 = embeddings[t1][0], embeddings[t2][0], embeddings[t3][0], embeddings[t4][0], embeddings[t5][0]
        print(top1,',', top2,',', top3,',', top4,',', top5)


else: # evaluation on held-out test set
    vo_dict = None
    with open("data/vo_dict.json") as f:
        for line in f:
            vo_dict = json.loads(line)
    with open(opt.input_test, 'r') as test_file:
        test_data = list(csv.reader(test_file)) #test data를 불러옴
        test_dt = []
        for row in test_data:
            affordances = str(row[3]) #vector를 가져옴
            sentence = process_command(row[2], word2id, id2word) #command에 해당하는 vector를 만듬
            #이 testset은 obj와 verb가 빠짐
            test_dt.append([row[0], row[1], sentence, affordances, row[4]])
        #generate retrieval tasks (from the held-out test data) to test the model
        ret_set = genRet(test_dt, vo_dict)
        ret(model, ret_set, id2word)
